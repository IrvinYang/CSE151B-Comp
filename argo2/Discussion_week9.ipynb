{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import pickle as pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"./\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "    f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "    #print(f_in)\n",
    "    inputs = pickle.load(open(f_in, \"rb\"))\n",
    "    inputs = np.asarray(inputs)\n",
    "    \n",
    "    outputs = None\n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        \n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)\n",
    "        \n",
    "    inputs = np.float32(inputs)\n",
    "    outputs = np.float32(outputs)\n",
    "    return inputs, outputs\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "        self.inputs, self.outputs = get_city_trajectories(city=city, split=split, normalized=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = (self.inputs[idx], self.outputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "# intialize a dataset\n",
    "city = 'palo-alto' \n",
    "split = 'train'\n",
    "train_dataset  = ArgoverseDataset(city = city, split = split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((43041, 50, 2), (43041, 60, 2))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization and Standardization For Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need to normalize time series?\n",
    "#### 1). Large input values can result in a model that learns large weight values. \n",
    "#### 2). Large target values result in large error gradient values causing weight values to change dramatically, making the learning process unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_mean = np.mean(inputs, axis = (0,1), keepdims = True)\n",
    "global_std = np.std(np.sqrt(inputs[:, :, 0]**2 + inputs[:, :, 0]**2))\n",
    "standard_inputs = (inputs - global_mean)/global_std\n",
    "standard_targets = (outputs - global_mean)/global_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  6270.,  19766., 346886., 662378., 628304., 465532., 248990.,\n",
       "        149815.,  51938.,   2581.]),\n",
       " array([-2.13738842, -1.67110405, -1.20481967, -0.7385353 , -0.27225093,\n",
       "         0.19403344,  0.66031781,  1.12660219,  1.59288656,  2.05917093,\n",
       "         2.5254553 ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(standard_targets[:,:,1].reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = model(standard_inputs)\n",
    "# loss_fun(predictions, standard_targets)\n",
    "# inverse_preds = predictions*global_std + global_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_min = np.min(inputs, axis = (0,1), keepdims = True)\n",
    "global_max = np.max(inputs, axis = (0,1), keepdims = True)\n",
    "normal_inputs = (inputs - global_min)/(global_max - global_min)\n",
    "normal_targets = (outputs - global_min)/(global_max - global_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_min = np.min(inputs, axis = (1), keepdims = True)\n",
    "traj_max = np.max(inputs, axis = (1), keepdims = True)\n",
    "normal_inputs = (inputs - traj_min)/(traj_max - traj_min)\n",
    "normal_targets = (outputs - traj_min)/(traj_max - traj_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecasting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_batch = torch.from_numpy(standard_inputs[:32]).float()\n",
    "batch_size = 10  # batch size \n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size, drop_last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Seq2Seq with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout_rate):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = input_dim, \n",
    "                            hidden_size = hidden_dim, \n",
    "                            num_layers= num_layers, \n",
    "                            dropout = dropout_rate, \n",
    "                            batch_first = True)\n",
    "        \n",
    "        \n",
    "    def forward(self, source):\n",
    "        \n",
    "        # hidden = (h, c)\n",
    "        # h, c: num_layers x bz x  hid_dim\n",
    "        # outputs: bz x input_length x hid_dim\n",
    "        outputs, hidden = self.lstm(source)\n",
    "        \n",
    "        return outputs, hidden\n",
    "    \n",
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, num_layers, dropout_rate):\n",
    "\n",
    "        super(AttnDecoder, self).__init__()\n",
    "\n",
    "        # Learn the attention scores\n",
    "        self.attn = nn.Linear(hidden_dim * num_layers+ output_dim, 50)\n",
    "        \n",
    "        # Learn the final input to the decoder \n",
    "        self.attn_combine = nn.Linear(hidden_dim + output_dim, hidden_dim)\n",
    "        \n",
    "        # Decoder LSTM\n",
    "        self.lstm = nn.LSTM(input_size = hidden_dim, \n",
    "                            hidden_size = hidden_dim, \n",
    "                            num_layers= num_layers, \n",
    "                            dropout = dropout_rate, \n",
    "                            batch_first = True)\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "      \n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        h = hidden[0]\n",
    "        h = h.transpose(0,1).reshape(h.shape[1], -1)\n",
    "\n",
    "        # Compute Attention Scores\n",
    "        energy = self.attn(torch.cat([x, h], 1))\n",
    "        attn_weights = F.softmax(energy, dim =1)\n",
    "        \n",
    "        # Calculate weighted sum of encoder hidden states\n",
    "        \n",
    "        attn_applied = torch.einsum(\"bl,lbh->bh\", attn_weights, encoder_outputs)\n",
    "        x = torch.cat((x, attn_applied), dim = 1)\n",
    "        x = self.attn_combine(x).unsqueeze(1)\n",
    "        x = F.relu(x)\n",
    "        output, decoder_hidden= self.lstm(x, hidden)  \n",
    "        prediction = self.output_layer(output.float())\n",
    "        \n",
    "        return prediction.squeeze(1), decoder_hidden\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, \n",
    "                 input_dim = 2, output_dim = 2, hidden_dim = 1024, num_layers = 2, dropout_rate = 0.1):\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target_length):\n",
    "\n",
    "        batch_size = 10\n",
    "        input_length = 50\n",
    "        \n",
    "        encoder_outputs, concat_hidden = self.encoder(source)\n",
    "\n",
    "        # the last encoder hidden state is used as initial hidden state of the decoder\n",
    "        decoder_hidden = concat_hidden\n",
    "        \n",
    "        # the first input to the decoder is last input position\n",
    "        decoder_output = source[:,-1]\n",
    "    \n",
    "        outputs = torch.zeros(batch_size, 60, 2)\n",
    "        for t in range(60):    \n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_output, decoder_hidden, encoder_outputs)\n",
    "            outputs[:,t] = decoder_output\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-ee6bb0b8c487>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mlewis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlewis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-144-ee6bb0b8c487>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, model, epochs, lr)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-143-b020e771fdf6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source, target_length)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-143-b020e771fdf6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Compute Attention Scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menergy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "def L2_2D(x, y):\n",
    "    #x1,x2 = zip(*inp)\n",
    "    #y1,y2 = zip(*out)\n",
    "    #x1, x2, y1, y2 = torch.tensor(x1), torch.tensor(x2), torch.tensor(y1), torch.tensor(y2)\n",
    "    #return sum(np.sqrt(np.square(x1-y1) + np.square(x2-y2)))\n",
    "    return ((x - y) ** 2).sum()\n",
    "    \n",
    "\n",
    "def train(encoder, decoder, model, epochs, lr):\n",
    "        \n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    lossF = L2_2D\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for X, y in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X.float(), y)\n",
    "            loss = lossF(torch.reshape(outputs, [60*batch_size, 2]), torch.reshape(y, [60*batch_size, 2]))\n",
    "            epoch_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f\"Epoch: {epoch+1} Loss:{epoch_loss/len(train_loader)}\")\n",
    "        \n",
    "    return model\n",
    "\n",
    "n_layers = 1\n",
    "lr = 50\n",
    "epochs = 30\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 128\n",
    "n_layers = 2\n",
    "dropout_p = 0.1\n",
    "output_size = 2\n",
    "\n",
    "encoder_net = Encoder(input_size, hidden_size, n_layers, dropout_p)\n",
    "decoder_net = AttnDecoder(output_size, hidden_size, n_layers, dropout_p)\n",
    "lewis = Seq2Seq(encoder_net, decoder_net)\n",
    "\n",
    "encoder_net.to(device)\n",
    "decoder_net.to(device)\n",
    "lewis.to(device)\n",
    "\n",
    "model = train(encoder_net, decoder_net, lewis, epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
