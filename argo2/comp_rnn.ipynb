{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the package dependencies before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import pickle\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Torch.Dataset class for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ROOT_PATH = \"./\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "\n",
    "\n",
    "def standardize(inputs, outputs):\n",
    "    m = np.mean(inputs, axis = (0,1), keepdims = True)\n",
    "    s = np.std(np.sqrt(inputs[:, :, 0]**2 + inputs[:, :, 0]**2))\n",
    "    standard_inputs = (inputs - m)/s\n",
    "    standard_targets = (outputs - m)/s\n",
    "    return standard_inputs, standard_targets, m, s\n",
    "\n",
    "def standardize_test(inputs):\n",
    "    m = np.mean(inputs, axis = (0,1), keepdims = True)\n",
    "    s = np.std(np.sqrt(inputs[:, :, 0]**2 + inputs[:, :, 0]**2))\n",
    "    standard_inputs = (inputs - m)/s\n",
    "    return standard_inputs\n",
    "\n",
    "\n",
    "\n",
    "def reverse_std(m, s, preds):\n",
    "    return preds * s + m\n",
    "\n",
    "\n",
    "\n",
    "def get_mean_std(city=\"palo-alto\", split=\"train\"):\n",
    "    f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "    inputs = pickle.load(open(f_in, \"rb\"))\n",
    "    inputs = np.asarray(inputs)\n",
    "    \n",
    "    outputs = None\n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)\n",
    "    \n",
    "    inputs, outputs, m, s = standardize(inputs, outputs)\n",
    "    \n",
    "    return m, s\n",
    "\n",
    "\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "    f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "    inputs = pickle.load(open(f_in, \"rb\"))\n",
    "    inputs = np.asarray(inputs)\n",
    "    inputs = np.float32(inputs)\n",
    "    \n",
    "    outputs = None\n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)\n",
    "        \n",
    "        #standardize\n",
    "        inputs, outputs, m, s = standardize(inputs, outputs)\n",
    "        outputs = np.float32(outputs)\n",
    "        return inputs, outputs\n",
    "\n",
    "    else:\n",
    "        return standardize_test(inputs), None\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "        self.inputs, self.outputs = get_city_trajectories(city=city, split=split, normalized=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.outputs is None:\n",
    "            data = self.inputs[idx]\n",
    "        else: \n",
    "            data = (self.inputs[idx], self.outputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "# intialize a dataset\n",
    "city = 'palo-alto' \n",
    "split = 'train'\n",
    "train_dataset  = ArgoverseDataset(city = city, split = split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DataLoader class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10 # batch size \n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size, drop_last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample a batch of data and visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "def show_sample_batch(sample_batch):\n",
    "    \"\"\"visualize the trajectory for a batch of samples\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        axs[i].xaxis.set_ticks([])\n",
    "        axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i,:,0], inp[i,:,1])\n",
    "        axs[i].scatter(out[i,:,0], out[i,:,1])\n",
    "\n",
    "        \n",
    "for i_batch, sample_batch in enumerate(train_loader):\n",
    "    inp, out = sample_batch\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      implement your Deep learning model\n",
    "      implement training routine\n",
    "    \"\"\"\n",
    "    \n",
    "    show_sample_batch(sample_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "X, Y = [], []\n",
    "for x, y in train_loader:\n",
    "    [[X.append(j.tolist()) for j in i] for i in x]\n",
    "    [[Y.append(j.tolist()) for j in i] for i in y]\n",
    "\n",
    "x1, x2 = [i[0] for i in X], [i[1] for i in X]\n",
    "y1, y2 = [i[0] for i in Y], [i[1] for i in Y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(x1, x2):\n",
    "    heatmap_x, xedges, yedges = np.histogram2d(x1, x2, bins=[100, 100])\n",
    "    extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "    plt.imshow(heatmap_x.T, extent=extent, origin='lower')\n",
    "    plt.show()\n",
    "graph(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph(y1, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could try bidirectional hidden \n",
    "# Could try different attention mechanisms\n",
    "# softmax layers and pooling layers\n",
    "# normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, n_layers, dropout_p):\n",
    "        super(encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #layers\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size,\n",
    "                            self.n_layers, dropout = dropout_p, batch_first = True)\n",
    "        #self.fc_hidden = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        #self.fc_cell = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        encoder_output, hidden = self.lstm(inp)\n",
    "        #hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n",
    "        #cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n",
    "        return encoder_output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, output_size, n_layers, dropout_p = 0.1):\n",
    "        super(decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # attention \n",
    "        self.attn = nn.Linear(hidden_size * n_layers + output_size, 50)\n",
    "        self.attn_combine = nn.Linear(hidden_size + output_size, hidden_size)\n",
    "        \n",
    "        #layers\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size,\n",
    "                            self.n_layers, dropout = dropout_p, batch_first = True)\n",
    "        \n",
    "        self.energy = nn.Linear(self.hidden_size * n_layers + output_size , 1)\n",
    "        self.out =  nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input_, encoder_states, hidden):\n",
    "        \n",
    "        h = hidden[0]\n",
    "        h = h.transpose(0,1).reshape(h.shape[1], -1)\n",
    "        \n",
    "        energy = self.attn(torch.cat([input_, h], 1))\n",
    "        attention = self.softmax(energy)\n",
    "                \n",
    "        # usesing mps makes the encoder_state take the shape of [50, batchsize, hidden_size]\n",
    "        if device == 'cpu':\n",
    "            string = \"bl,blh->bh\"\n",
    "        else: string = \"bl,lbh->bh\"\n",
    "        context_vector = torch.einsum(string, attention, encoder_states)\n",
    "        \n",
    "        rnn_input = torch.cat((input_, context_vector), dim=1)\n",
    "        rnn_input = self.attn_combine(rnn_input).unsqueeze(1)\n",
    "        rnn_input = self.relu(rnn_input)\n",
    "\n",
    "        output, decoder_hidden = self.lstm(rnn_input, hidden)\n",
    "        output = self.out(output.float())\n",
    "        \n",
    "        return output.squeeze(1), decoder_hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(model, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        \n",
    "        batch_size = source.shape[0]\n",
    "        target_len = 60\n",
    "\n",
    "        outputs = torch.zeros(batch_size, 60, 2).to(device)\n",
    "        \n",
    "        encoder_states, encoder_hidden = self.encoder(source)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        x = source[:, -1]\n",
    "        for t in range(target_len):\n",
    "            output, decoder_hidden = self.decoder(x, encoder_states, decoder_hidden)\n",
    "            outputs[:,t] = output\n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_2D(x, y):\n",
    "    #x1,x2 = zip(*inp)\n",
    "    #y1,y2 = zip(*out)\n",
    "    #x1, x2, y1, y2 = torch.tensor(x1), torch.tensor(x2), torch.tensor(y1), torch.tensor(y2)\n",
    "    #return sum(np.sqrt(np.square(x1-y1) + np.square(x2-y2)))\n",
    "    return ((x - y) ** 2).sum()\n",
    "    \n",
    "\n",
    "def train(encoder, decoder, model, epochs, lr):\n",
    "    \n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    lossF = L2_2D\n",
    "    \n",
    "    loss_history = [np.inf]\n",
    "    prev_model = None\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        for X, y in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X.float(), y)\n",
    "            loss = lossF(torch.reshape(outputs, [60*batch_size, 2]), torch.reshape(y, [60*batch_size, 2]))\n",
    "            epoch_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(city + ' ' + f\"Epoch: {epoch+1} Loss:{epoch_loss/len(train_loader)}\")\n",
    "        \n",
    "        #early stopping\n",
    "        if epoch_loss > (loss_history[-1] * 1.25):\n",
    "            return prev_model, outputs\n",
    "        \n",
    "        loss_history.append(epoch_loss)\n",
    "        prev_model = model\n",
    "        \n",
    "        \n",
    "    return model, loss_history\n",
    "\n",
    "def pred(model, test_data):\n",
    "    \n",
    "    t = torch.tensor([i for i in test_data])\n",
    "    model.eval()\n",
    "    t = t.to(device)\n",
    "    fake_target = torch.zeros(len(test_data), 60, 2).to(device)\n",
    "    outputs = model(t.float(), fake_target)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train by cities:\n",
    "\n",
    "\n",
    "\n",
    "#universal parameters\n",
    "lr = 0.002\n",
    "epochs = 30\n",
    "input_size = 2\n",
    "hidden_size = 128\n",
    "n_layers = 1\n",
    "dropout_p = 0.1\n",
    "output_size = 2\n",
    "\n",
    "#logs\n",
    "models = {}\n",
    "loss_histories = {}\n",
    "preidctions = {}\n",
    "\n",
    "\n",
    "for city in cities:\n",
    "    \n",
    "    split = 'train'\n",
    "    \n",
    "    #loaders\n",
    "    train_data = ArgoverseDataset(city = city, split = split)\n",
    "    batch_size = 10\n",
    "    train_loader = DataLoader(train_dataset,batch_size=batch_size, drop_last = True)\n",
    "    \n",
    "    #setup new model for each city\n",
    "    encoder_net = encoder(input_size, hidden_size, n_layers, dropout_p)\n",
    "    decoder_net = decoder(hidden_size, output_size, n_layers, dropout_p)\n",
    "    lewis = model(encoder_net, decoder_net)\n",
    "    encoder_net.to(device)\n",
    "    decoder_net.to(device)\n",
    "    lewis.to(device)\n",
    "    \n",
    "    #training\n",
    "    lewis, loss_history = train(encoder_net, decoder_net, lewis, epochs, lr)\n",
    "    \n",
    "    loss_histories[city] = loss_history\n",
    "    models[city] = lewis\n",
    "    \n",
    "\n",
    "    split = 'test'\n",
    "    test_data = ArgoverseDataset(city = city, split = split)\n",
    "    preds = pred(lewis, test_data)\n",
    "    m, s = get_mean_std(city=city, split=\"train\")\n",
    "    preds = preds.detach().numpy()\n",
    "    preds = reverse_std(m, s, preds)\n",
    "    predictions[city] = preds\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
